{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training : Box office prediction\n",
        "\n",
        "This codebook documents the model training and evaluation pipeline for the Box Office 7-Day Domestic Revenue Prediction project. Building on the preprocessed and feature-engineered dataset, this notebook is responsible for constructing reproducible train–validation–test splits and training multiple predictive models to benchmark performance and understand model behavior.\n",
        "\n",
        "The codebook implements three complementary modeling approaches: a linear regression as a transparent baseline, LightGBM as a high-performance gradient boosting model for capturing non-linear interactions, and CatBoost for robust handling of categorical features and complex relationships. Each model is trained under consistent data splits and evaluated using standardized error metrics to enable fair comparison.\n",
        "\n",
        "The goal of this codebook is twofold: first, to establish a reliable baseline against which more complex models can be judged, and second, to assess how much predictive lift is gained from non-linear and ensemble-based methods. Together, these experiments provide both interpretability and performance insights, guiding model selection for accurate and economically meaningful box office revenue forecasting.\n",
        "\n",
        "Finally, the best fitting model is used to predict the domestic weekly box office value generated by Avatar:Fire and Ash"
      ],
      "metadata": {
        "id": "rcnv7fM7-gcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##All imports"
      ],
      "metadata": {
        "id": "24qHp67M-wR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPR9VmS6Ss8q"
      },
      "outputs": [],
      "source": [
        "# Install once (quiet)\n",
        "!pip install -q catboost optuna lightgbm\n",
        "\n",
        "# Core\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Models & metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Feature list"
      ],
      "metadata": {
        "id": "WDPI6Q8C-zkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Box office prediction dataset/features.csv')\n"
      ],
      "metadata": {
        "id": "-cDyxr7_S3XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "VsVKGJLCTBVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Y Variable"
      ],
      "metadata": {
        "id": "vb3wmkIm-4lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=df['Seven_Day_Total']"
      ],
      "metadata": {
        "id": "vQbpZ2fTeHxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "n_oUwaexg555"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date-time conversions that were missed in preprocessing"
      ],
      "metadata": {
        "id": "tfcb8Qd8-9K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "EzLqKcnMYTay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Derive numeric year (int) for splitting\n",
        "df[\"year\"] = df[\"release_date\"].dt.year"
      ],
      "metadata": {
        "id": "lxCRSSw7YLzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing Step: Creating Covid Flat\n",
        "This flag was an afterthought hence created later to account for the pressure on movie theaters during covid-19 pandemic"
      ],
      "metadata": {
        "id": "kmIph2Cp_Dbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"covid_era\"] = df[\"year\"].between(2020, 2021).astype(int)\n"
      ],
      "metadata": {
        "id": "6UxhDlwjY-TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating X: Final feature list"
      ],
      "metadata": {
        "id": "cAqywe7Q_VkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=df[['Distributor','Theaters','runtime','log_total_marketing_assets',\n",
        "     'director_weighted_rating','weekend_release','is_holiday','holiday_type','spoken_languages_iso','has_same_day_competitor',\n",
        "     'same_day_competitors','month_sin','month_cos','budget_log','cast_top5_popularity_sum_log','is_franchise','franchise_size_log'\n",
        "     ,'installment_number','is_sequel','keywords_count_log','kw_superhero','kw_fantasy_scifi','kw_horror','kw_animation','kw_romance','kw_biopic'\n",
        "     ,'kw_crime_thriller','kw_comedy_drama','kw_action_adventure','kw_family','genres_norm','num_genres','num_production_companies','has_major_studio',\n",
        "     'num_prodcos_log','mpaa_std','is_US','is_China','is_India','num_countries','covid_era','year','Seven_Day_Total']]"
      ],
      "metadata": {
        "id": "h8zCeYXJfE1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.info()"
      ],
      "metadata": {
        "id": "zVo_pRxWkIq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.info()"
      ],
      "metadata": {
        "id": "pLHBPPrXkJqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train- Valid-Test split"
      ],
      "metadata": {
        "id": "NByL8GGF_cqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Time-based split\n",
        "train_df = x[x[\"year\"] <= 2019]\n",
        "val_df   = x[(x[\"year\"] >= 2020) & (x[\"year\"] <= 2022)]\n",
        "test_df  = x[x[\"year\"] >= 2023]\n",
        "\n",
        "# 4) Features/target\n",
        "TARGET = \"Seven_Day_Total\"\n",
        "X_train, y_train = train_df.drop(columns=[TARGET]), train_df[TARGET]\n",
        "X_val,   y_val   = val_df.drop(columns=[TARGET]),   val_df[TARGET]\n",
        "X_test,  y_test  = test_df.drop(columns=[TARGET]),  test_df[TARGET]\n",
        "\n",
        "# 5) Sanity checks\n",
        "print(\"Train years:\", int(X_train[\"year\"].min()), \"-\", int(X_train[\"year\"].max()), \"| rows:\", len(X_train))\n",
        "print(\"Val years:  \", int(X_val[\"year\"].min()),   \"-\", int(X_val[\"year\"].max()),   \"| rows:\", len(X_val))\n",
        "print(\"Test years: \", int(X_test[\"year\"].min()),  \"-\", int(X_test[\"year\"].max()),  \"| rows:\", len(X_test))\n"
      ],
      "metadata": {
        "id": "LpdCii_VkMhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Totals add up (excluding rows with missing year)\n",
        "n_all = df[\"year\"].notna().sum()\n",
        "n_train, n_val, n_test = len(X_train), len(X_val), len(X_test)\n",
        "print(\"Covered rows:\", n_train + n_val + n_test, \"of\", n_all)\n",
        "\n",
        "# 2) Year ranges are disjoint and ordered\n",
        "print(\"Train years unique:\", sorted(train_df[\"year\"].unique()))\n",
        "print(\"Val years unique:  \", sorted(val_df[\"year\"].unique()))\n",
        "print(\"Test years unique: \", sorted(test_df[\"year\"].unique()))\n",
        "\n",
        "# 3) No overlap of indices\n",
        "print(\"Overlap train∩val:\", len(set(train_df.index) & set(val_df.index)))\n",
        "print(\"Overlap val∩test: \", len(set(val_df.index) & set(test_df.index)))\n",
        "print(\"Overlap train∩test:\", len(set(train_df.index) & set(test_df.index)))\n",
        "\n",
        "# 4) Any NaT release dates that were dropped by the filters?\n",
        "print(\"Rows with NaT release_date:\", df[\"release_date\"].isna().sum())\n",
        "\n",
        "# 5) Target availability check\n",
        "for name, part in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    missing_target = part[\"Seven_Day_Total\"].isna().sum()\n",
        "    print(f\"{name}: missing Seven_Day_Total = {missing_target}\")\n",
        "\n",
        "# 6) Optional distribution sanity checks\n",
        "for name, part in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    print(f\"{name}: mean target={part['Seven_Day_Total'].mean():.2f}, \"\n",
        "          f\"median={part['Seven_Day_Total'].median():.2f}, \"\n",
        "          f\"n={len(part)}\")\n"
      ],
      "metadata": {
        "id": "eodqQ0hjm5YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CatBoost Regressor:\n",
        "CatBoost Regressor was chosen because it **handles categorical features natively**, reducing the need for extensive one-hot encoding and lowering the risk of information loss. It is particularly effective with **heterogeneous tabular data**, which fits the mix of financial, categorical, and engineered features in this dataset. CatBoost is **robust to overfitting**, especially on smaller-to-medium datasets, due to its ordered boosting strategy. It captures **non-linear interactions** between features that linear models cannot. Finally, it delivers strong performance with **minimal feature scaling and preprocessing**, making it well-suited for this problem.\n"
      ],
      "metadata": {
        "id": "wQMO8ZvYA30P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Before hyper-params tuning"
      ],
      "metadata": {
        "id": "FNwmIPghBP5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log-transform the target for stability\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_valid_log = np.log1p(y_val)\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Fill NaN values in categorical columns with a placeholder string\n",
        "# CatBoost requires categorical features to be strings or integers, not NaN\n",
        "X_train_processed = X_train.copy()\n",
        "X_val_processed = X_val.copy()\n",
        "for col in cat_cols:\n",
        "    X_train_processed[col] = X_train_processed[col].fillna('')\n",
        "    X_val_processed[col] = X_val_processed[col].fillna('')\n",
        "\n",
        "cat_feature_indices = [X_train_processed.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "train_pool = Pool(\n",
        "    data=X_train_processed,\n",
        "    label=y_train_log,\n",
        "    cat_features=cat_feature_indices\n",
        ")\n",
        "valid_pool = Pool(\n",
        "    data=X_val_processed,\n",
        "    label=y_valid_log,\n",
        "    cat_features=cat_feature_indices\n",
        ")\n",
        "\n",
        "# Reasonable starting hyperparameters\n",
        "cb = CatBoostRegressor(\n",
        "    loss_function=\"RMSE\",         # in log-space because labels are log1p\n",
        "    eval_metric=\"RMSE\",\n",
        "    depth=8,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=3.0,\n",
        "    n_estimators=5000,\n",
        "    random_seed=42,\n",
        "    od_type=\"Iter\",               # early stopping\n",
        "    od_wait=200,\n",
        "    verbose=200\n",
        ")\n",
        "\n",
        "cb.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "\n",
        "# Predict and invert log\n",
        "valid_pred_log = cb.predict(valid_pool)\n",
        "valid_pred = np.expm1(valid_pred_log)\n",
        "\n",
        "# Metrics\n",
        "rmse_log = np.sqrt(mean_squared_error(y_valid_log, valid_pred_log))\n",
        "mae = mean_absolute_error(y_val, valid_pred)\n",
        "\n",
        "print(f\"Validation RMSE (log-space): {rmse_log:.4f}\")\n",
        "print(f\"Validation MAE (original scale): {mae:,.0f}\")\n",
        "\n",
        "# Feature importance\n",
        "fi = pd.Series(cb.get_feature_importance(train_pool), index=X_train_processed.columns).sort_values(ascending=False)\n",
        "print(fi.head(25))"
      ],
      "metadata": {
        "id": "oz2M-GJVpZGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Model Interpretation\n",
        "\n",
        "The model was stopped at 780 iterations, indicating early stopping once validation performance stopped improving, which helps prevent overfitting.\n",
        "\n",
        "A Validation RMSE (log-space) of 1.1437 suggests moderate predictive accuracy on log-transformed revenue; errors grow multiplicatively rather than additively.\n",
        "\n",
        "A Validation MAE of ~11.0 million (original scale) means that, on average, predictions differ from actual 7-day revenue by about USD 11M.\n",
        "\n",
        "Feature importance values indicate how strongly each variable contributes to reducing prediction error, not directionality.\n",
        "\n",
        "Feature-by-Feature Explanation (Descending Importance)\n",
        "1. Theaters (56.20)\n",
        "\n",
        "The strongest predictor. Opening theater count directly constrains revenue potential—wider releases almost always generate higher opening-week grosses. This dominates all other signals.\n",
        "\n",
        "2. Distributor (5.08)\n",
        "\n",
        "Encodes distribution strength, release strategy, and bargaining power with exhibitors. Major distributors typically secure more screens, better timing, and stronger marketing support.\n",
        "\n",
        "3. budget_log (4.74)\n",
        "\n",
        "Budget reflects production scale and expected commercial ambition. The log transform captures diminishing returns while still signaling blockbuster-level investment.\n",
        "\n",
        "4. log_total_marketing_assets (3.98)\n",
        "\n",
        "Represents overall marketing intensity. Strongly predictive, but secondary to theaters and budget, aligning with real-world box office dynamics.\n",
        "\n",
        "5. keywords_count_log (3.10)\n",
        "\n",
        "Acts as a proxy for narrative complexity, genre richness, and discoverability. More keywords often reflect higher audience targeting and content depth.\n",
        "\n",
        "6. director_weighted_rating (3.08)\n",
        "\n",
        "Captures historical credibility and audience trust in the director. Well-rated directors consistently improve opening performance.\n",
        "\n",
        "7. runtime (2.92)\n",
        "\n",
        "Runtime affects daily screening capacity and audience perception. Both very short and very long runtimes carry trade-offs the model learns non-linearly.\n",
        "\n",
        "8. mpaa_std (2.46)\n",
        "\n",
        "Rating impacts addressable audience size (e.g., PG-13 vs R). This is a meaningful constraint on revenue, especially for family-driven openings.\n",
        "\n",
        "9. cast_top5_popularity_sum_log (2.45)\n",
        "\n",
        "Star power matters, but with diminishing returns. The log scale reflects that once a cast is highly popular, incremental fame adds less value.\n",
        "\n",
        "10. year (2.01)\n",
        "\n",
        "Captures macro trends such as inflation, market growth/decline, streaming competition, and post-pandemic effects.\n",
        "\n",
        "11. month_sin (1.73)\n",
        "\n",
        "Encodes seasonal release cycles. Certain periods (summer, holidays) systematically outperform others.\n",
        "\n",
        "12. franchise_size_log (1.37)\n",
        "\n",
        "Larger franchises carry built-in audience awareness, but benefits taper as franchises grow longer.\n",
        "\n",
        "13. month_cos (1.15)\n",
        "\n",
        "Complements month_sin to fully encode cyclical seasonality without artificial ordering.\n",
        "\n",
        "14. holiday_type (1.12)\n",
        "\n",
        "Opening near major holidays boosts attendance due to extended leisure time and family viewing.\n",
        "\n",
        "15. installment_number (0.92)\n",
        "\n",
        "Later installments behave differently than originals—often front-loaded but sometimes fatigued—captured here numerically.\n",
        "\n",
        "16. num_genres (0.87)\n",
        "\n",
        "Genre breadth can widen appeal, but excessive mixing may dilute targeting.\n",
        "\n",
        "17. genres_norm (0.86)\n",
        "\n",
        "Specific genre combinations influence audience size and expectations beyond just count.\n",
        "\n",
        "18. num_countries (0.83)\n",
        "\n",
        "International production involvement can signal higher budgets or global appeal, even when predicting domestic revenue.\n",
        "\n",
        "19. num_production_companies (0.75)\n",
        "\n",
        "More producers often indicate larger-scale projects, though with limited marginal gain.\n",
        "\n",
        "20. num_prodcos_log (0.70)\n",
        "\n",
        "Log version confirms diminishing returns—useful, but weaker than raw scale effects.\n",
        "\n",
        "21. has_major_studio (0.67)\n",
        "\n",
        "Major studio backing provides structural advantages, though much of its effect overlaps with theaters and distributor.\n",
        "\n",
        "22. spoken_languages_iso (0.65)\n",
        "\n",
        "Language diversity influences accessibility but is less critical for domestic opening performance.\n",
        "\n",
        "23. same_day_competitors (0.63)\n",
        "\n",
        "Competition slightly depresses revenue, but its effect is smaller than release scale and marketing.\n",
        "\n",
        "24. is_franchise (0.46)\n",
        "\n",
        "Binary franchise membership matters less than how big or which installment the franchise is.\n",
        "\n",
        "25. weekend_release (0.44)\n",
        "\n",
        "Most films already open on weekends, limiting its discriminative power."
      ],
      "metadata": {
        "id": "8tlljC0oBhrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CatBoost: Categorical Prep & Pool Construction (log-target)"
      ],
      "metadata": {
        "id": "QZhudFCuCJqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Explicit list of categoricals used with CatBoost\n",
        "cat_cols = [c for c in [\"Distributor\", \"holiday_type\", \"spoken_languages_iso\", \"genres_norm\", \"mpaa_std\"]\n",
        "            if c in X_train.columns]\n",
        "\n",
        "# 2) Sanitize categoricals in each split: fill NaN and cast to string\n",
        "for frame in (X_train, X_val, X_test):\n",
        "    for c in cat_cols:\n",
        "        # Replace NaN/NaT with a placeholder, then ensure string dtype\n",
        "        frame[c] = frame[c].where(frame[c].notna(), \"Unknown\").astype(str)\n",
        "\n",
        "# (Optional) Quick check that no NaNs remain in categoricals\n",
        "print(X_train[cat_cols].isna().sum())\n",
        "print(X_val[cat_cols].isna().sum())\n",
        "print(X_test[cat_cols].isna().sum())\n",
        "\n",
        "# 3) Recompute categorical indices after any column changes\n",
        "cat_idx = [X_train.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "# 4) Build Pools safely\n",
        "from catboost import Pool\n",
        "import numpy as np\n",
        "\n",
        "y_train_log = np.log1p(y_train.astype(float))\n",
        "y_val_log   = np.log1p(y_val.astype(float))\n",
        "y_test_log  = np.log1p(y_test.astype(float))\n",
        "\n",
        "train_pool = Pool(X_train, label=y_train_log, cat_features=cat_idx)\n",
        "valid_pool = Pool(X_val,   label=y_val_log,   cat_features=cat_idx)\n",
        "test_pool  = Pool(X_test,  label=y_test_log,  cat_features=cat_idx)\n"
      ],
      "metadata": {
        "id": "rGyaM87YuH6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyper- parameter tuning: CatBoost Regressor"
      ],
      "metadata": {
        "id": "JH94rty2CVAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        \"loss_function\": \"RMSE\",\n",
        "        \"eval_metric\":   \"RMSE\",\n",
        "        \"random_seed\":   42,\n",
        "        \"depth\": trial.suggest_int(\"depth\", 6, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.2, log=True),\n",
        "        \"l2_leaf_reg\":   trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, log=True),\n",
        "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
        "        \"random_strength\":     trial.suggest_float(\"random_strength\", 0.0, 2.0),\n",
        "        \"rsm\":                 trial.suggest_float(\"rsm\", 0.7, 1.0),\n",
        "        \"border_count\":        trial.suggest_int(\"border_count\", 128, 254),\n",
        "        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\"]),\n",
        "        \"od_type\": \"Iter\",\n",
        "        \"od_wait\": 200,\n",
        "        \"verbose\": False,\n",
        "    }\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 1500, 6000, step=500)\n",
        "\n",
        "    model = CatBoostRegressor(**params, n_estimators=n_estimators)\n",
        "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "\n",
        "    val_pred_log = model.predict(valid_pool)\n",
        "    rmse_log = np.sqrt(mean_squared_error(y_val_log, val_pred_log))  # <-- patched\n",
        "\n",
        "    trial.report(rmse_log, step=0)\n",
        "    if trial.should_prune():\n",
        "        raise optuna.TrialPruned()\n",
        "\n",
        "    return rmse_log\n"
      ],
      "metadata": {
        "id": "pL1nG9WTvD8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"minimize\", study_name=\"catboost_boxoffice\")\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)  # adjust trials as you wish\n",
        "\n",
        "print(\"Best value (val RMSE log):\", study.best_value)\n",
        "print(\"Best params:\", study.best_params)\n"
      ],
      "metadata": {
        "id": "O-9ciIVuvPTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After Hyper-params tuning: CatBoost Regressor"
      ],
      "metadata": {
        "id": "40BeSXL2Cpwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine train + valid for final fit\n",
        "X_trv = pd.concat([X_train, X_val], axis=0)\n",
        "y_trv = pd.concat([y_train, y_val], axis=0)\n",
        "y_trv_log = np.log1p(y_trv.astype(float))\n",
        "\n",
        "cat_cols = [c for c in X_trv.columns if X_trv[c].dtype == \"object\"]\n",
        "cat_idx  = [X_trv.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "trv_pool  = Pool(X_trv,  label=y_trv_log,  cat_features=cat_idx)\n",
        "test_pool = Pool(X_test, label=y_test_log, cat_features=cat_idx)\n",
        "\n",
        "best_params = study.best_params.copy()\n",
        "best_n_estimators = best_params.pop(\"n_estimators\")\n",
        "\n",
        "final_model = CatBoostRegressor(\n",
        "    loss_function=\"RMSE\",\n",
        "    eval_metric=\"RMSE\",\n",
        "    random_seed=42,\n",
        "    od_type=\"Iter\",\n",
        "    od_wait=200,\n",
        "    verbose=200,\n",
        "    n_estimators=best_n_estimators,\n",
        "    **best_params\n",
        ")\n",
        "\n",
        "final_model.fit(trv_pool, eval_set=test_pool, use_best_model=True)\n",
        "\n",
        "# Evaluate on TEST\n",
        "test_pred_log = final_model.predict(test_pool)\n",
        "test_pred     = np.expm1(test_pred_log)\n",
        "\n",
        "# Metrics (compat with older sklearn: compute RMSE via sqrt(MSE))\n",
        "rmse_log = np.sqrt(mean_squared_error(y_test_log, test_pred_log))\n",
        "mae      = mean_absolute_error(y_test, test_pred)\n",
        "rmse     = np.sqrt(mean_squared_error(y_test, test_pred))\n",
        "\n",
        "# R-squared (report both spaces)\n",
        "r2_log   = r2_score(y_test_log, test_pred_log)   # on log target\n",
        "r2_orig  = r2_score(y_test,     test_pred)       # on original scale\n",
        "\n",
        "print(f\"TEST RMSE (log-space): {rmse_log:.4f}\")\n",
        "print(f\"TEST R^2  (log-space): {r2_log:.4f}\")\n",
        "print(f\"TEST MAE  (original) : {mae:,.0f}\")\n",
        "print(f\"TEST RMSE (original) : {rmse:,.0f}\")\n",
        "print(f\"TEST R^2  (original) : {r2_orig:.4f}\")\n"
      ],
      "metadata": {
        "id": "B_jhEY3gwxQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tuned CatBoost model achieved a test RMSE of 0.74 in log-space, corresponding to a typical prediction error within a factor of approximately two. The model explains nearly 89% of the variance in log first-week revenue and approximately 58% in the original revenue scale. These results substantially outperform earlier baselines and indicate strong predictive signal under a strictly time-aware evaluation protocol."
      ],
      "metadata": {
        "id": "OJGW15doxnyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hLPiflrbC5sC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling and Encoding"
      ],
      "metadata": {
        "id": "_D0cZmqwCyny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# ---- inputs you already have ----\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Explicit categoricals (single-label)\n",
        "CAT_COLS = [c for c in [\"Distributor\", \"holiday_type\", \"spoken_languages_iso\", \"mpaa_std\"] if c in X_train.columns]\n",
        "\n",
        "# Multi-label column (pipe-separated string, e.g., \"Action|Adventure\")\n",
        "GENRES_COL = \"genres_norm\" if \"genres_norm\" in X_train.columns else None\n",
        "TOP_K_GENRES = 12  # adjust if you want wider/narrower\n",
        "\n",
        "# Optional: cap long-tail categories to \"Other\" based on TRAIN distribution only\n",
        "def cap_categories_train_only(x_tr, x_va, x_te, col, top_n=30):\n",
        "    top = x_tr[col].value_counts().head(top_n).index\n",
        "    x_tr[col] = np.where(x_tr[col].isin(top), x_tr[col], \"Other\")\n",
        "    x_va[col] = np.where(x_va[col].isin(top), x_va[col], \"Other\")\n",
        "    x_te[col] = np.where(x_te[col].isin(top), x_te[col], \"Other\")\n",
        "    return x_tr, x_va, x_te\n",
        "\n",
        "# Sanitize categoricals: fill NaN → \"Unknown\", cast to str\n",
        "def clean_cats_inplace(df, cols):\n",
        "    for c in cols:\n",
        "        df[c] = df[c].where(df[c].notna(), \"Unknown\").astype(str)"
      ],
      "metadata": {
        "id": "etvMDFOD7FfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to avoid SettingWithCopy surprises\n",
        "Xtr, Xva, Xte = X_train.copy(), X_val.copy(), X_test.copy()\n",
        "\n",
        "# Clean + cap single-label categoricals\n",
        "clean_cats_inplace(Xtr, CAT_COLS)\n",
        "clean_cats_inplace(Xva, CAT_COLS)\n",
        "clean_cats_inplace(Xte, CAT_COLS)\n",
        "\n",
        "for c in CAT_COLS:\n",
        "    Xtr, Xva, Xte = cap_categories_train_only(Xtr, Xva, Xte, c, top_n=30)\n",
        "\n",
        "# One-Hot Encoder on TRAIN only\n",
        "ohe=OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False,drop=None)\n",
        "ohe.fit(Xtr[CAT_COLS])\n",
        "\n",
        "def apply_ohe(df):\n",
        "    arr = ohe.transform(df[CAT_COLS])\n",
        "    cols = ohe.get_feature_names_out(CAT_COLS)\n",
        "    ohe_df = pd.DataFrame(arr, columns=cols, index=df.index)\n",
        "    return pd.concat([df.drop(columns=CAT_COLS), ohe_df], axis=1)\n",
        "\n",
        "Xtr_ohe = apply_ohe(Xtr)\n",
        "Xva_ohe = apply_ohe(Xva)\n",
        "Xte_ohe = apply_ohe(Xte)\n",
        "\n",
        "ohe_cols = list(ohe.get_feature_names_out(CAT_COLS))\n"
      ],
      "metadata": {
        "id": "U-rUVVY78PY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_genre_space(xtr, col, k=TOP_K_GENRES):\n",
        "    # build top-K list from TRAIN only\n",
        "    g = (\n",
        "        xtr[col].fillna(\"\")\n",
        "        .astype(str).str.split(\"|\")\n",
        "        .explode().str.strip()\n",
        "    )\n",
        "    top = g[g != \"\"].value_counts().head(k).index.tolist()\n",
        "    return top\n",
        "\n",
        "def add_genre_multihot(df, col, top_genres):\n",
        "    if col is None:\n",
        "        return df, []\n",
        "    base = df[col].fillna(\"\").astype(str)\n",
        "    out_cols = []\n",
        "    for g in top_genres:\n",
        "        cname = f\"genre_{g}\"\n",
        "        df[cname] = base.str.contains(fr\"(?:^|\\|){g}(?:\\||$)\", regex=True).astype(int)\n",
        "        out_cols.append(cname)\n",
        "    return df.drop(columns=[col], errors=\"ignore\"), out_cols\n",
        "\n",
        "if GENRES_COL:\n",
        "    top_genres = fit_genre_space(Xtr_ohe, GENRES_COL, k=TOP_K_GENRES)\n",
        "    Xtr_ohe, genre_cols = add_genre_multihot(Xtr_ohe, GENRES_COL, top_genres)\n",
        "    Xva_ohe, _         = add_genre_multihot(Xva_ohe, GENRES_COL, top_genres)\n",
        "    Xte_ohe, _         = add_genre_multihot(Xte_ohe, GENRES_COL, top_genres)\n",
        "else:\n",
        "    genre_cols = []\n"
      ],
      "metadata": {
        "id": "P7GLovI48g16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_cols = Xtr_ohe.columns.tolist()\n",
        "\n",
        "non_ohe_cols = [c for c in all_cols if c not in ohe_cols + genre_cols]\n",
        "num_cols_lin = [c for c in non_ohe_cols if np.issubdtype(Xtr_ohe[c].dtype, np.number)]\n",
        "\n",
        "# Standardize numerics for LINEAR only (fit on TRAIN, apply to VAL/TEST)\n",
        "scaler = StandardScaler()\n",
        "Xtr_lin = Xtr_ohe.copy()\n",
        "Xva_lin = Xva_ohe.copy()\n",
        "Xte_lin = Xte_ohe.copy()\n",
        "\n",
        "Xtr_lin[num_cols_lin] = scaler.fit_transform(Xtr_lin[num_cols_lin])\n",
        "Xva_lin[num_cols_lin] = scaler.transform(Xva_lin[num_cols_lin])\n",
        "Xte_lin[num_cols_lin] = scaler.transform(Xte_lin[num_cols_lin])\n"
      ],
      "metadata": {
        "id": "hm0xxHNL8nHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For LightGBM (tree-based), use the unscaled OHE+multi-hot frames:\n",
        "Xtr_lgbm, Xva_lgbm, Xte_lgbm = Xtr_ohe, Xva_ohe, Xte_ohe"
      ],
      "metadata": {
        "id": "bvzyWtcs8pOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline: LinearRegression"
      ],
      "metadata": {
        "id": "YIGIHsVKDIKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ytr_log = np.log1p(y_train.astype(float))\n",
        "yva_log = np.log1p(y_val.astype(float))\n",
        "yte_log = np.log1p(y_test.astype(float))\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(Xtr_lin, ytr_log)\n",
        "\n",
        "# Validation\n",
        "pred_va_log = lr.predict(Xva_lin)\n",
        "pred_va = np.expm1(pred_va_log)\n",
        "\n",
        "rmse_log_va = np.sqrt(mean_squared_error(yva_log, pred_va_log))\n",
        "mae_va      = mean_absolute_error(y_val, pred_va)\n",
        "r2_log_va   = r2_score(yva_log, pred_va_log)\n",
        "\n",
        "print(f\"[Baseline Linear] VAL RMSE (log): {rmse_log_va:.4f} | VAL MAE: {mae_va:,.0f} | VAL R^2 (log): {r2_log_va:.4f}\")\n",
        "\n",
        "# Test\n",
        "pred_te_log = lr.predict(Xte_lin)\n",
        "pred_te = np.expm1(pred_te_log)\n",
        "\n",
        "rmse_log_te = np.sqrt(mean_squared_error(yte_log, pred_te_log))\n",
        "mae_te      = mean_absolute_error(y_test, pred_te)\n",
        "r2_log_te   = r2_score(yte_log, pred_te_log)\n",
        "\n",
        "print(f\"[Baseline Linear] TEST RMSE (log): {rmse_log_te:.4f} | TEST MAE: {mae_te:,.0f} | TEST R^2 (log): {r2_log_te:.4f}\")\n"
      ],
      "metadata": {
        "id": "CadQnau88rt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lightgbm Regressor"
      ],
      "metadata": {
        "id": "ituixc2JDjZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Before hyper parameters tuning"
      ],
      "metadata": {
        "id": "i3stCFQFDoiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to sanitize column names\n",
        "def sanitize_lgbm_col_names(df):\n",
        "    cols = df.columns\n",
        "    new_cols = []\n",
        "    for col in cols:\n",
        "        new_col = re.sub(r'[^A-Za-z0-9_]+', '_', col)\n",
        "        new_col = new_col.replace(' ', '_')\n",
        "        new_cols.append(new_col)\n",
        "    df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "# Sanitize column names for LightGBM DataFrames\n",
        "Xtr_lgbm_sanitized = sanitize_lgbm_col_names(Xtr_lgbm.copy())\n",
        "Xva_lgbm_sanitized = sanitize_lgbm_col_names(Xva_lgbm.copy())\n",
        "Xte_lgbm_sanitized = sanitize_lgbm_col_names(Xte_lgbm.copy())\n",
        "\n",
        "# LightGBM works fine on OHE + raw numerics; we keep the log-target\n",
        "lgb_train = lgb.Dataset(Xtr_lgbm_sanitized, label=ytr_log)\n",
        "lgb_valid = lgb.Dataset(Xva_lgbm_sanitized, label=yva_log, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": \"rmse\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 63,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"min_data_in_leaf\": 20,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "lgbm_model = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=5000,\n",
        "    valid_sets=[lgb_train, lgb_valid],\n",
        "    valid_names=[\"train\",\"valid\"],\n",
        "    callbacks=[lgb.early_stopping(200, verbose=200)]\n",
        ")\n",
        "\n",
        "# Evaluate on TEST (log-space)\n",
        "pred_te_log_lgbm = lgbm_model.predict(Xte_lgbm_sanitized, num_iteration=lgbm_model.best_iteration)\n",
        "pred_te_lgbm = np.expm1(pred_te_log_lgbm)\n",
        "\n",
        "rmse_log_te_lgbm = np.sqrt(mean_squared_error(yte_log, pred_te_log_lgbm))\n",
        "mae_te_lgbm      = mean_absolute_error(y_test, pred_te_lgbm)\n",
        "r2_log_te_lgbm   = r2_score(yte_log, pred_te_log_lgbm)\n",
        "\n",
        "print(f\"[LightGBM] TEST RMSE (log): {rmse_log_te_lgbm:.4f} | TEST MAE: {mae_te_lgbm:,.0f} | TEST R^2 (log): {r2_log_te_lgbm:.4f}\")"
      ],
      "metadata": {
        "id": "jG7CKt-pJ4LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyper-parameter tuning and Final LightGBM training"
      ],
      "metadata": {
        "id": "j_3JIl1aERUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING)  # quiet, ASCII-safe\n",
        "\n",
        "# Function to sanitize column names\n",
        "def sanitize_lgbm_col_names(df):\n",
        "    cols = df.columns\n",
        "    new_cols = []\n",
        "    for col in cols:\n",
        "        new_col = re.sub(r'[^A-Za-z0-9_]+', '_', col)\n",
        "        new_col = new_col.replace(' ', '_')\n",
        "        new_cols.append(new_col)\n",
        "    df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "# Sanitize column names for LightGBM DataFrames\n",
        "Xtr_lgbm = sanitize_lgbm_col_names(Xtr_lgbm.copy())\n",
        "Xva_lgbm = sanitize_lgbm_col_names(Xva_lgbm.copy())\n",
        "Xte_lgbm = sanitize_lgbm_col_names(Xte_lgbm.copy())\n",
        "\n",
        "lgb_train = lgb.Dataset(Xtr_lgbm, label=ytr_log)\n",
        "lgb_valid = lgb.Dataset(Xva_lgbm, label=yva_log, reference=lgb_train)\n",
        "\n",
        "def objective(trial: optuna.Trial) -> float:\n",
        "    params = {\n",
        "        \"objective\": \"regression\",\n",
        "        \"metric\": \"rmse\",\n",
        "        \"verbosity\": -1,                  # no unicode logs\n",
        "        \"seed\": 42,\n",
        "        \"feature_pre_filter\": False,      # Add this line to resolve the LightGBMError\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 16),\n",
        "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 200),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-4, 10.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-4, 10.0, log=True),\n",
        "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.0, 1.0),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 7),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
        "        \"extra_trees\": trial.suggest_categorical(\"extra_trees\", [False, True]),\n",
        "    }\n",
        "    num_boost_round = trial.suggest_int(\"num_boost_round\", 1500, 6000, step=500)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        lgb_train,\n",
        "        num_boost_round=num_boost_round,\n",
        "        valid_sets=[lgb_valid],           # only valid set\n",
        "        valid_names=[\"valid\"],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
        "            lgb.log_evaluation(period=0), # no periodic logs\n",
        "        ],\n",
        "    )\n",
        "    return model.best_score[\"valid\"][\"rmse\"]\n",
        "\n",
        "# Use a pruner, no progress bar\n",
        "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
        "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
        "\n",
        "# Safe ASCII prints\n",
        "print(\"Best val log-RMSE:\", study.best_value)\n",
        "print(\"Best params:\", json.dumps(study.best_params, ensure_ascii=True))\n",
        "\n",
        "# Retrain on train+valid (quiet)\n",
        "X_trv = np.ascontiguousarray(np.vstack([Xtr_lgbm, Xva_lgbm]))\n",
        "y_trv_log = np.concatenate([ytr_log, yva_log])\n",
        "lgb_trv = lgb.Dataset(X_trv, label=y_trv_log)\n",
        "\n",
        "best = study.best_params.copy()\n",
        "num_boost_round = best.pop(\"num_boost_round\")\n",
        "\n",
        "final_model = lgb.train(\n",
        "    {**best, \"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1, \"seed\": 42},\n",
        "    lgb_trv,\n",
        "    num_boost_round=num_boost_round,\n",
        "    valid_sets=[lgb_trv],\n",
        "    valid_names=[\"train_valid\"],\n",
        "    callbacks=[lgb.log_evaluation(period=0)],\n",
        ")\n",
        "\n",
        "# Test evaluation (explicit sqrt for older sklearn)\n",
        "pred_test_log = final_model.predict(Xte_lgbm, num_iteration=final_model.best_iteration)\n",
        "pred_test = np.expm1(pred_test_log)\n",
        "\n",
        "rmse_log = np.sqrt(mean_squared_error(yte_log, pred_test_log))\n",
        "mae      = mean_absolute_error(y_test, pred_test)\n",
        "rmse     = np.sqrt(mean_squared_error(y_test, pred_test))\n",
        "r2_log   = r2_score(yte_log, pred_test_log)\n",
        "r2_orig  = r2_score(y_test, pred_test)\n",
        "\n",
        "print(\"TEST_RMSE_LOG:\", float(rmse_log))\n",
        "print(\"TEST_R2_LOG:\",  float(r2_log))\n",
        "print(\"TEST_MAE:\",     float(mae))\n",
        "print(\"TEST_RMSE:\",    float(rmse))\n",
        "print(\"TEST_R2:\",      float(r2_orig))"
      ],
      "metadata": {
        "id": "xyG0ILstMRJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Metric**                     | **Linear Regression (Baseline)** | **CatBoost Regressor** | **LightGBM Regressor** |\n",
        "| ------------------------------ | -------------------------------- | ---------------------- | ---------------------- |\n",
        "| **TEST RMSE (log-space)**      | 0.9539                           | **0.7626**             | 0.7746                 |\n",
        "| **TEST R² (log-space)**        | 0.8136                           | **0.8809**             | 0.8771                 |\n",
        "| **TEST MAE (original scale)**  | 16,330,805                       | **10,484,272**         | 10,570,600             |\n",
        "| **TEST RMSE (original scale)** | N/A                              | **27,656,886**         | 28,378,886             |\n",
        "| **TEST R² (original scale)**   | N/A                              | **0.5539**             | 0.5303                 |\n"
      ],
      "metadata": {
        "id": "cQ9RMnCxO7bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Selection: CatBoost Regressor\n",
        "\n",
        "CatBoost was selected as the ideal model because it consistently delivered the **best overall predictive performance** while remaining well-suited to the structure of the dataset. Compared to Linear Regression, CatBoost captured complex non-linear relationships and interactions inherent in box office dynamics, and it marginally outperformed LightGBM across both log-space and original-scale metrics. Its native handling of categorical features such as distributor, holiday type, MPAA rating, genres, and languages reduced the need for aggressive encoding and helped preserve informative signals. CatBoost’s ordered boosting strategy also limited overfitting, leading to stronger generalization under a time-aware train–test split. Together, these properties allowed CatBoost to achieve the lowest error and highest explanatory power, making it the most reliable and interpretable choice for first-week box office revenue prediction in this project.\n"
      ],
      "metadata": {
        "id": "8i6hVFwOEyMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Avatar : Fire and Ash data for prediction"
      ],
      "metadata": {
        "id": "r6qX0ra1E5qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This avatar data was preprocessed the same as the dataset. As the code for the preprocessing of this data was exactly the same, the codebook to be referred to is Data-preprocessing. The original codebook for the csv file is not shared."
      ],
      "metadata": {
        "id": "ssU5AWlDFdkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avatar_pred=pd.read_csv(\"/content/drive/MyDrive/Box office prediction dataset/avatar_features.csv\")"
      ],
      "metadata": {
        "id": "ecImx7WLOAnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avatar_pred.info()"
      ],
      "metadata": {
        "id": "Aax7prkEMxp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first set of code blocks prepares x_avatar as a model-ready inference sample by mirroring the exact feature structure used during training. It engineers a covid_era indicator from the release year, selects the finalized feature set in the same order as the training matrix, and renames release_year to year to ensure schema consistency. The code then applies categorical sanitation (clean_cats_inplace) to fill missing values and enforce string types, followed by category capping using thresholds learned only from the training data. This guarantees that unseen or rare categories in the Avatar sample are safely mapped to \"Other\" without leaking test-time information into the model.\n",
        "\n",
        "The second block constructs a CatBoost Pool for inference using the previously computed categorical feature indices, which tells CatBoost how to correctly process categorical columns. The tuned CatBoost model (cb) then predicts log-transformed first-week revenue for the Avatar sample. Finally, the prediction is converted back to the original dollar scale using np.expm1, producing an interpretable revenue estimate. Together, these steps ensure that inference is deterministic, leakage-free, and fully aligned with the training pipeline, allowing the Avatar prediction to be evaluated with confidence."
      ],
      "metadata": {
        "id": "DOmSUTNzGUMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avatar_pred[\"covid_era\"] = avatar_pred[\"release_year\"].between(2020, 2021).astype(int)"
      ],
      "metadata": {
        "id": "qDNDrYJaM-1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_avatar=avatar_pred[['Distributor','Theaters','runtime','log_total_marketing_assets',\n",
        "     'director_weighted_rating','weekend_release','is_holiday','holiday_type','spoken_languages_iso','has_same_day_competitor',\n",
        "     'same_day_competitors','month_sin','month_cos','budget_log','cast_top5_popularity_sum_log','is_franchise','franchise_size_log'\n",
        "     ,'installment_number','is_sequel','keywords_count_log','kw_superhero','kw_fantasy_scifi','kw_horror','kw_animation','kw_romance','kw_biopic'\n",
        "     ,'kw_crime_thriller','kw_comedy_drama','kw_action_adventure','kw_family','genres_norm','num_genres','num_production_companies','has_major_studio',\n",
        "     'num_prodcos_log','mpaa_std','is_US','is_China','is_India','num_countries','covid_era','release_year',]]"
      ],
      "metadata": {
        "id": "clk1rYD8MtSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_avatar='Seven_Day_Total'"
      ],
      "metadata": {
        "id": "1Wl9HXmjPHMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6143fa92"
      },
      "source": [
        "x_avatar = x_avatar.rename(columns={'release_year': 'year'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59cf96f2"
      },
      "source": [
        "clean_cats_inplace(x_avatar, CAT_COLS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a919355a"
      },
      "source": [
        "for c in CAT_COLS:\n",
        "    X_train, x_avatar, X_test = cap_categories_train_only(X_train, x_avatar, X_test, c, top_n=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4b3e2a2"
      },
      "source": [
        "avatar_pool = Pool(x_avatar, cat_features=cat_idx)\n",
        "print(\"CatBoost Pool for x_avatar created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predicted Revenue for Avatar"
      ],
      "metadata": {
        "id": "GVA2XgyaKSoC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f768f53b"
      },
      "source": [
        "predicted_log_revenue = cb.predict(avatar_pool)\n",
        "predicted_revenue = np.expm1(predicted_log_revenue)\n",
        "\n",
        "print(f\"Predicted revenue for 'Avatar: Fire and Ash': ${predicted_revenue[0]:,.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$124,989,450.64 is the model’s point estimate of first-7-day domestic box office for “Avatar: Fire and Ash.” The CatBoost model predicts log(revenue + 1); your code then applies np.expm1() to invert that log so the result is in dollars.\n",
        "\n",
        "Why the model landed here\n",
        "CatBoost learned relationships from 2,573 past releases and combined the inputs you fed for this title—e.g., Theaters (release scale), Distributor/major-studio flag, budget_log, log_total_marketing_assets, franchise signals (is_franchise, franchise_size_log, installment_number), seasonality/holiday features (month_sin/cos, holiday_type), and same-day competition—into a nonlinear forecast. In your feature importances, Theaters dominates, with material lift from Distributor, budget_log, and marketing intensity; those likely pushed the prediction into the ~$125M range, moderated by timing (month/holiday), competition, runtime, and content features.\n",
        "\n",
        "How to interpret accuracy\n",
        "The project’s tuned CatBoost shows RMSE ≈ 0.76 in log space and MAE ≈ $10.5M on the original scale. Practically, that means forecasts are typically within ~$10–11M of actuals but can deviate more for outliers (raw-scale RMSE ≈ $27–28M). So a reasonable expectation band is roughly $125M ± $10M on average, acknowledging wider uncertainty for exceptional blockbusters."
      ],
      "metadata": {
        "id": "zY1BpdoDJe5V"
      }
    }
  ]
}